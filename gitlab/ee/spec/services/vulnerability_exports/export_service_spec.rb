# frozen_string_literal: true

require 'spec_helper'

RSpec.describe VulnerabilityExports::ExportService, feature_category: :vulnerability_management do
  let_it_be(:group) { create(:group, name: "ðŸ”’ gitlab", path: 'gitlab') }
  let_it_be(:project_a) { create(:project, group: group) }

  describe '::export' do
    let(:vulnerability_export) { create(:vulnerability_export) }
    let(:mock_service_object) { instance_double(described_class, export: true) }

    subject(:export) { described_class.export(vulnerability_export) }

    before do
      allow(described_class).to receive(:new).and_return(mock_service_object)
    end

    it 'instantiates a new instance of the service class and sends export message to it' do
      export

      expect(described_class).to have_received(:new).with(vulnerability_export)
      expect(mock_service_object).to have_received(:export)
    end
  end

  describe '#export_segment' do
    let_it_be(:subgroup_1) { create(:group, parent: group) }
    let_it_be(:subgroup_2) { create(:group, parent: group) }
    let_it_be(:subgroup_1_project) { create(:project, group: subgroup_1) }
    let_it_be(:subgroup_2_project) { create(:project, group: subgroup_2) }

    let_it_be(:vulnerability_1) { create(:vulnerability, :dismissed, :with_read, project: subgroup_2_project) }
    let_it_be(:vulnerability_2) { create(:vulnerability, :with_read, project: subgroup_1_project) }

    let!(:vulnerability_export) { create(:vulnerability_export, :created, group: group, project: nil) }
    let!(:vulnerability_export_part) do
      create(
        :vulnerability_export_part,
        vulnerability_export: vulnerability_export,
        start_id: vulnerability_2.id,
        end_id: vulnerability_1.id
      )
    end

    let(:service_object) { described_class.new(vulnerability_export) }

    subject(:export_segment) { service_object.export_segment(vulnerability_export_part) }

    it 'generates the exported segment file' do
      expect(VulnerabilityExports::ExportDeletionWorker).not_to receive(:perform_async)

      export_segment

      csv = CSV.read(vulnerability_export_part.reload.file.path, headers: true)

      expect(csv.headers).to be_present
      expect(csv['Vulnerability']).to match_array([vulnerability_1.title, vulnerability_2.title])
      expect(csv['Status']).to match_array([vulnerability_1.state, vulnerability_2.state])
    end

    context 'when the export fails' do
      before do
        allow(vulnerability_export_part).to receive(:start_id).and_raise(RuntimeError)
      end

      it 'raises an error and cleans up the export' do
        expect(VulnerabilityExports::ExportDeletionWorker).to receive(:perform_in).with(1.hour, vulnerability_export.id)

        expect { export_segment }.to raise_error(RuntimeError)

        expect(vulnerability_export.reload.status).to eq('failed')
      end
    end
  end

  describe '#finalise_segmented_export' do
    let!(:vulnerability_export) { create(:vulnerability_export, :running, group: group, project: nil) }
    let!(:vulnerabilities) { create_list(:vulnerability, 3, :with_read, project: project_a) }
    let!(:vulnerbility_export_part_1) do
      create(
        :vulnerability_export_part,
        vulnerability_export: vulnerability_export,
        start_id: vulnerabilities.first.id,
        end_id: vulnerabilities.second.id,
        file: write_tempfile("dontshow,\n123,456,789\n")
      )
    end

    let!(:vulnerbility_export_part_2) do
      create(
        :vulnerability_export_part,
        vulnerability_export: vulnerability_export,
        start_id: vulnerabilities.last.id,
        end_id: vulnerabilities.last.id,
        file: write_tempfile("dontshow,\n987,654,321\n")
      )
    end

    let(:vulnerability_export_parts) { [vulnerbility_export_part_1, vulnerbility_export_part_2] }
    let(:service_object) { described_class.new(vulnerability_export) }

    subject(:finalise_segmented_export) { service_object.finalise_segmented_export }

    it_behaves_like 'large segmented file export' do
      let(:export) { vulnerability_export }
    end

    it 'merges the exported segments into one file, dropping excess headers' do
      expect(VulnerabilityExports::ExportDeletionWorker).to receive(:perform_in).with(1.hour, vulnerability_export.id)
      expect(vulnerability_export).to receive(:export_parts).and_return(vulnerability_export_parts)
      expect(vulnerbility_export_part_1).to receive(:file).and_call_original
      expect(vulnerbility_export_part_2).to receive(:file).and_call_original
      allow(service_object).to receive(:export_header).and_return("headerline,headerline,headerline,\n")

      expect { finalise_segmented_export }.to change { vulnerability_export.reload.file.filename }

      expect(vulnerability_export.reload.file.read).to eq(
        "headerline,headerline,headerline,\n123,456,789\n987,654,321\n"
      )
      expect(vulnerability_export.reload.status).to eq('finished')
    end

    context 'when the export fails' do
      before do
        allow_next_instance_of(Tempfile) do |tempfile|
          allow(tempfile).to receive(:<<).and_raise(RuntimeError)
        end
      end

      it 'raises an error and cleans up the export' do
        expect(VulnerabilityExports::ExportDeletionWorker).to receive(:perform_in).with(1.hour, vulnerability_export.id)

        expect { finalise_segmented_export }.to raise_error(RuntimeError)

        expect(vulnerability_export.reload.status).to eq('failed')
      end
    end

    describe 'writing content into final tempfile concurrently' do
      before do
        allow(service_object).to receive(:export_header).and_return("headerline\n")

        # In this test, there are two fibers(one fiber per export part) concurrently writing
        # content to a temporary file.
        # By mocking all the I/O related operations, we prevent FiberScheduler from transferring
        # execution from one to another. This makes the execution deterministic so the fiber for
        # the first export part always reaches the critical section before the other one and holds
        # the semaphore.
        mock_stream_1 = instance_double(StringIO, readline: true)
        mock_stream_2 = instance_double(StringIO, readline: true)

        allow(vulnerability_export.export_parts[0].file).to receive(:open).and_yield(mock_stream_1)
        allow(vulnerability_export.export_parts[1].file).to receive(:open).and_yield(mock_stream_2)

        allow(mock_stream_1).to receive(:each_line).and_yield(+"foo\n")
        allow(mock_stream_2).to receive(:each_line).and_yield(+"bar\n")

        call_count = 0

        allow_next_instance_of(Tempfile) do |tempfile|
          allow(tempfile).to receive(:<<).and_wrap_original do |original, *args|
            call_count += 1

            # Here we are deliberately transfering execution to second fiber from the first one after holding the
            # semaphore in critical section.
            # This simulates the case where I/O write operation(`Tempfile#<<`) calling the FiberScheduler#io_write
            # which transfers the execution to another fiber.
            #
            # When we call `FiberScheduler#yield` from the first fiber within the critical section after holding
            # the semaphore, the scheduler transfers the execution to the second fiber. Second fiber starts running
            # and waits for the semaphore to be available as it's already held by the first one. This notifies the
            # scheduler and it transfers the execution back to first fiber. Once the first fiber starts running again,
            # it writes content to tempfile and completes, then the second fiber starts running from where it's left
            # and writes its content to the tempfile.
            Fiber.scheduler.yield if call_count == 1

            original.call(*args)
          end
        end
      end

      it 'synchonizes the write operations' do
        finalise_segmented_export

        expect(vulnerability_export.reload.file.read).to eq(
          "headerline\nfoo\nbar\n"
        )
      end
    end
  end

  describe '#export' do
    let!(:vulnerability_export) { create(:vulnerability_export, :created) }
    let(:service_object) { described_class.new(vulnerability_export) }

    subject(:export) { service_object.export }

    context 'generating the export file' do
      let(:lease_name) { "vulnerability_exports_export:#{vulnerability_export.id}" }

      before do
        allow(service_object).to receive(:in_lock)
      end

      it 'runs synchronized with distributed semaphore' do
        export

        expect(service_object).to have_received(:in_lock).with(lease_name, ttl: 1.hour)
      end
    end

    context 'when the vulnerability_export is not in `created` state' do
      before do
        allow(vulnerability_export).to receive(:created?).and_return(false)
        allow(service_object).to receive(:generate_export)
      end

      it 'does not execute export file generation logic' do
        export

        expect(service_object).not_to have_received(:generate_export)
      end
    end

    context 'when the vulnerability_export is in `created` state' do
      context 'when the exportable is a group' do
        let!(:vulnerability_export) { create(:vulnerability_export, :created, group: group, project: nil) }

        context 'when the vulnerabilities are more than the partial file batch size' do
          let!(:vulnerabilities) { create_list(:vulnerability, 3, :with_read, project: project_a) }
          let(:segmented_export_workers_count) { 5 }
          let(:vuln_export_1) { instance_double(::Vulnerabilities::Export::Part, id: 1234) }
          let(:vuln_export_2) { instance_double(::Vulnerabilities::Export::Part, id: 4321) }

          before do
            stub_const('VulnerabilityExports::ExportService::VULNERABILITY_READS_PARTIAL_FILE_BATCH_SIZE', 2)
            stub_const('VulnerabilityExports::ExportService::SEGMENTED_EXPORT_WORKERS', segmented_export_workers_count)
          end

          it 'generates the export file in batches, scaling workers to the number of parts needed' do
            expect(::Vulnerabilities::Export::Part).to receive(:create).with(
              vulnerability_export_id: vulnerability_export.id,
              start_id: vulnerabilities.first.id,
              end_id: vulnerabilities.second.id,
              organization_id: vulnerability_export.organization_id
            ).and_return(vuln_export_1)
            expect(::Vulnerabilities::Export::Part).to receive(:create).with(
              vulnerability_export_id: vulnerability_export.id,
              start_id: vulnerabilities.last.id,
              end_id: vulnerabilities.last.id,
              organization_id: vulnerability_export.organization_id
            ).and_return(vuln_export_2)

            expect(::Gitlab::Export::SegmentedExportWorker).to receive(:perform_async).with(
              vulnerability_export.to_global_id,
              [vuln_export_1.id]
            )
            expect(::Gitlab::Export::SegmentedExportWorker).to receive(:perform_async).with(
              vulnerability_export.to_global_id,
              [vuln_export_2.id]
            )

            export
          end

          it 'sets organization_id appropriately' do
            expect { export }.to change { ::Vulnerabilities::Export::Part.count }.from(0).to(2)

            expect(::Vulnerabilities::Export::Part.first.organization_id).to eq(vulnerability_export.organization_id)
            expect(::Vulnerabilities::Export::Part.last.organization_id).to eq(vulnerability_export.organization_id)
          end

          context 'when there are no vulnerabilities for the vulnerable' do
            let(:group) { create(:group) }

            it 'does not raise an error' do
              expect(::Gitlab::Export::SegmentedExportFinalisationWorker).to receive(:perform_async).with(
                vulnerability_export.to_global_id
              )

              expect { export }.not_to raise_error
            end
          end

          context 'when an error occurs during the enqueuing' do
            before do
              allow(::Gitlab::Export::SegmentedExportWorker).to receive(:perform_async).and_raise(StandardError)
            end

            it 'raises an error and cleans up the export' do
              expect(VulnerabilityExports::ExportDeletionWorker).to receive(:perform_in).with(1.hour, vulnerability_export.id)
              expect { export }.to raise_error(StandardError)
              expect(vulnerability_export.reload.status).to eq('failed')
            end
          end

          context 'when there are many vulnerabilities' do
            let!(:more_vulnerabilities) { create_list(:vulnerability, 6, project: project_a) }
            let(:segmented_export_workers_count) { 1 }
            let!(:organization) { create(:organization, :default) }

            it 'does not create more export workers than SEGMENTED_EXPORT_WORKERS' do
              expect(::Gitlab::Export::SegmentedExportWorker).to receive(:perform_async).exactly(
                segmented_export_workers_count
              ).times

              export
            end
          end
        end
      end

      context 'when the exportable is a project' do
        before do
          allow(VulnerabilityExports::ExportDeletionWorker).to receive(:perform_in)
        end

        context 'when the export generation fails' do
          let(:error) { RuntimeError.new('foo') }

          before do
            allow(service_object).to receive(:generate_export_file).and_raise(error)
          end

          it 'sets the state of export back to `created`' do
            expect { export }.to raise_error(error)
            expect(vulnerability_export.reload.created?).to be_truthy
          end

          it 'schedules the export deletion background job' do
            expect { export }.to raise_error(error)
            expect(VulnerabilityExports::ExportDeletionWorker).to have_received(:perform_in).with(1.hour, vulnerability_export.id)
          end
        end

        context 'when the export generation succeeds' do
          before do
            allow(service_object).to receive(:generate_export_file)
            allow(vulnerability_export).to receive(:start!)
            allow(vulnerability_export).to receive(:finish!)
          end

          it 'marks the state of export object as `started` and then `finished`' do
            export

            expect(vulnerability_export).to have_received(:start!).ordered
            expect(vulnerability_export).to have_received(:finish!).ordered
          end

          it 'schedules the export deletion background job' do
            export

            expect(VulnerabilityExports::ExportDeletionWorker).to have_received(:perform_in).with(1.hour, vulnerability_export.id)
          end
        end

        context 'when the export format is csv' do
          let(:vulnerabilities) { Vulnerabilities::Read.none }
          let(:mock_relation) { double(:relation, with_findings_scanner_identifiers_and_notes: vulnerabilities) }
          let(:mock_vulnerability_finder_service_object) { instance_double(Security::VulnerabilityReadsFinder, execute: mock_relation) }
          let(:exportable_full_path) { 'foo' }
          let(:time_suffix) { Time.current.utc.strftime('%FT%H%M') }
          let(:expected_file_name) { "#{exportable_full_path}_vulnerabilities_#{time_suffix}.csv" }

          before do
            allow(Security::VulnerabilityReadsFinder).to receive(:new).and_return(mock_vulnerability_finder_service_object)
            allow(vulnerability_export.exportable).to receive(:full_path).and_return(exportable_full_path)
          end

          around do |example|
            freeze_time { example.run }
          end

          it 'calls the VulnerabilityExports::Exporters::CsvService which sets the file and filename' do
            expect { export }.to change { vulnerability_export.file }
                            .and change { vulnerability_export.file&.filename }.from(nil).to(expected_file_name)
          end
        end
      end
    end
  end

  # Using Tempfile.open closes the stream after the block, making the reference useless.
  def write_tempfile(body)
    f = Tempfile.new
    f << body
    f.rewind
    f
  end
end
